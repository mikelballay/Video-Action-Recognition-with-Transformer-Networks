import os
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertConfig
from sklearn.metrics import accuracy_score, confusion_matrix
from collections import defaultdict

# Define the model class (same as during training)
class VideoTransformer(nn.Module):
    def __init__(self, embedding_dim, num_labels, max_seq_length):
        super(VideoTransformer, self).__init__()
        config = BertConfig(
            hidden_size=embedding_dim,
            num_attention_heads=8,
            intermediate_size=embedding_dim * 4,
            num_hidden_layers=4,
            max_position_embeddings=max_seq_length,
            type_vocab_size=1,
            hidden_dropout_prob=0.1,
            attention_probs_dropout_prob=0.1,
        )
        self.bert = BertModel(config)
        self.classifier = nn.Linear(embedding_dim, num_labels)
        # Positional embeddings
        self.position_embeddings = nn.Embedding(max_seq_length, embedding_dim)
        self.register_buffer('position_ids', torch.arange(max_seq_length).expand((1, -1)))
        
    def forward(self, embeddings):
        position_ids = self.position_ids[:, :embeddings.size(1)].to(embeddings.device)
        position_embeddings = self.position_embeddings(position_ids)
        embeddings = embeddings + position_embeddings
        attention_mask = (embeddings.abs().sum(dim=-1) > 1e-4).long()
        outputs = self.bert(inputs_embeds=embeddings, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]
        logits = self.classifier(cls_output)
        return logits

# Define the dataset class (similar to the one used during training)
class PaddedVideoEmbeddingDataset(Dataset):
    def __init__(self, root_dir, labels, max_seq_length=200, embedding_dim=1024):
        self.npy_files = []
        for label in os.listdir(root_dir):
            class_dir = os.path.join(root_dir, label)
            if os.path.isdir(class_dir):
                files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.npy')]
                self.npy_files.extend(files)
        self.labels = labels
        self.max_seq_length = max_seq_length
        self.embedding_dim = embedding_dim
    
    def __len__(self):
        return len(self.npy_files)
    
    def __getitem__(self, idx):
        embeddings = np.load(self.npy_files[idx])  # Shape: (num_frames, embedding_dim)

        # Normalize embeddings
        embeddings = (embeddings - embeddings.mean(axis=0)) / (embeddings.std(axis=0) + 1e-8)
        
        # Pad or truncate to max_seq_length
        if embeddings.shape[0] > self.max_seq_length:
            embeddings = embeddings[:self.max_seq_length, :]
        else:
            padding = np.zeros((self.max_seq_length - embeddings.shape[0], self.embedding_dim))
            embeddings = np.vstack((embeddings, padding))

        # Extract label name from the file path
        file_path = self.npy_files[idx]
        label_name = os.path.basename(os.path.dirname(file_path))
        label = self.labels[label_name]
        
        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)

# Load label mappings (should be the same as during training)
# Replace with your actual label_to_index mapping
labels_list = [name for name in os.listdir('/home/mikel.ballay/cap4773_mikel/try/DARPA/ecole/train/') if os.path.isdir(os.path.join('/home/mikel.ballay/cap4773_mikel/try/DARPA/ecole/train/', name))]
label_to_index = {label: idx for idx, label in enumerate(sorted(labels_list))}
index_to_label = {index: label for label, index in label_to_index.items()}
num_labels = len(label_to_index)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the saved model
embedding_dim = 1024
max_seq_length = 200
model = VideoTransformer(embedding_dim, num_labels, max_seq_length).to(device)
model.load_state_dict(torch.load('model.pth', map_location=device))
model.eval()  # Set model to evaluation mode

# Path to the validation dataset
validation_root_dir = '/home/mikel.ballay/cap4773_mikel/try/DARPA_embeddings/val'  # Replace with your validation dataset path

# Create the validation dataset and dataloader
validation_dataset = PaddedVideoEmbeddingDataset(
    root_dir=validation_root_dir,
    labels=label_to_index,
    max_seq_length=max_seq_length,
    embedding_dim=embedding_dim
)
validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)
# Function to evaluate the model on the validation dataset
def evaluate(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for embeddings, labels in dataloader:
            embeddings = embeddings.to(device)
            labels = labels.to(device)
            outputs = model(embeddings)
            _, preds = torch.max(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    # Compute overall accuracy
    accuracy = accuracy_score(all_labels, all_preds)
    
    # Compute per-class accuracy
    per_class_accuracy = compute_per_class_accuracy(all_labels, all_preds, index_to_label)
    
    return accuracy, per_class_accuracy, all_labels, all_preds

def compute_per_class_accuracy(all_labels, all_preds, index_to_label):
    # Initialize dictionaries to hold counts
    per_class_correct = defaultdict(int)
    per_class_total = defaultdict(int)
    
    # Iterate over all labels and predictions
    for true_label, pred_label in zip(all_labels, all_preds):
        per_class_total[true_label] += 1
        if true_label == pred_label:
            per_class_correct[true_label] += 1
    
    # Calculate per-class accuracy
    per_class_accuracy = {}
    for label in per_class_total:
        accuracy = per_class_correct[label] / per_class_total[label]
        label_name = index_to_label[label]
        per_class_accuracy[label_name] = accuracy * 100  # Convert to percentage
    
    return per_class_accuracy

# Evaluate the model and print the accuracy
accuracy, per_class_accuracy, all_labels, all_preds = evaluate(model, validation_loader)
print(f"Overall Validation Accuracy: {accuracy * 100:.2f}%")

# Print per-class accuracy
print("\nPer-Class Accuracy:")
for label_name, acc in per_class_accuracy.items():
    print(f"Class '{label_name}': {acc:.2f}%")
